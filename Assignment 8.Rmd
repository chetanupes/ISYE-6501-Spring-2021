---
title: "Assignment 8"
output: pdf_document
---
## Question 11.1

Using the crime data set uscrime.txt from Questions 8.2, 9.1, and 10.1, build a regression model using:
1.	Stepwise regression
2.	Lasso
3.	Elastic net
For Parts 2 and 3, remember to scale the data first – otherwise, the regression coefficients will be on different scales and the constraint won’t have the desired effect.

## 1. Stepwise Regression

```{r message=FALSE, warning=FALSE}
library(MASS)
library(caret)
library(glmnet)
```

```{r message=TRUE, warning=FALSE}
#Getting the data
crime_data<-read.table('uscrime.txt', sep = "", header = TRUE)

#Set seed 
set.seed(42)

#Fitting the full model
lmr<-lm(Crime~., data = crime_data)
summary(lmr)

#Using Stepwise regression and evaluating the results
step<-stepAIC(lmr, scope = list(lower = Crime~1, upper = Crime~.),
              direction = 'both',trace = 1)
#Model summary
summary(step)
step$anova

#We can see that the step model predicts 8 predictors for the best model. 
#Now we will build a new regression model using just the 8 predictors and 
#evaluate the model

#New model
new<-lm(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob, data = crime_data)

#New model summary
summary(new)

#Now to further evaluate this model let use cross validation 
#Linear regression model using cross validation 
set.seed(123)
train.control <- trainControl(method = "cv", number = 5)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
               data = crime_data, method='lm',  #Linear Model2 with CV
               trControl=train.control)          
# CV Model summary
summary(model) 

#Note: The summary shows M.F and U1 are not significant based on the p-value
#(0.05). We can further test this by removing both M.F and U1

#New model
new_1<-lm(Crime ~ M + Ed + Po1 + U2 + Ineq + Prob, data = crime_data)

#New model summary
summary(new_1)

#Now to further evaluate this model let use cross validation 
#Linear regression model using cross validation 
set.seed(123)
train.control <- trainControl(method = "cv", number = 5)
model1<-train(Crime ~ M + Ed + Po1 + U2 + Ineq + Prob,
             data = crime_data, method='lm',  #Linear Model2 with CV
             trControl=train.control)  

# CV Model summary
summary(model1) 

```
## Conclusion: Stepwise Regression

The whole purpose of stepwise regression is to eliminate insignificant factors 
and make the model simpler. This should not be at the cost of losing important 
variables.We can see that the new model with only 6 factors produced an adjusted 
R2 of 73%. The previous model with all the 8 factors produced an adjusted R2 of
74%. So, the metrics for both models is close so we might as well keep the 8 
factors i n our model.

## 2. LASSO

```{r message=TRUE, warning=FALSE}

#Let first separate response and predictor for the algorithm
X<-crime_data[,1:15]
y<-crime_data[16]

#We need to also scale the data except the response data
X_scale<-scale(X)

# Setting alpha = 1 implements Lasso regressions
set.seed(42)
lasso_cv <- cv.glmnet(x=as.matrix(X_scale), y=as.matrix(y), alpha = 1,
                     nfolds = 5,type.measure="mse",family="gaussian")

# Plot cross-validation results
plot(lasso_cv)

#Lambda that gives minimum mean cross-validated error
lasso_cv$lambda.min

#Selected Lambda and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.min)

#Now using the above variables from the Lasso regression results and run a new 
#regression model

new_from_lasso<-lm(Crime ~ M+So+Ed+Po1+M.F+NW+U1+U2+Ineq+Prob+Wealth, 
                   data = crime_data)
summary(new_from_lasso)

#Further based on the p-value we can remove So,M.F,NW,U1,Wealth and run a new model
mod<-lm(Crime ~ M+Ed+Po1+U2+Ineq+Prob, 
        data = crime_data)
summary(mod)

#The following model have all variables with significant values so we can keep 
#it.Also, it is the same model we got through stepwise regression
```
## Conclusion: LASSO

We used the Lasso algorithm where the alpha value is 1 for variable selection. 
The model gave 11 significant variables and these were subsequently used to 
build a regression model. Further the p-value was used to eliminate the 
insignificant variable and a final regression model was build which was the same 
as stepwise regression.


## 2. Elastic Net

```{r message=TRUE, warning=FALSE}

#For elastic net we use different values of alpha and see which model gives 
#us the best result.

# Build the model using the values of alpha between 1-0

metric<-list()
rng<-as.list(seq(from = 0, to = 100, by = 1))

for (i in 1:length(rng)) {
  set.seed(42)
  elastic<-cv.glmnet(x=as.matrix(X_scale), y=as.matrix(y),
                          alpha=i/100,nfolds = 10)
  metric[i]=elastic$glmnet.fit$dev.ratio[which(elastic$glmnet.fit$lambda==
                                                        elastic$lambda.min)]
}

# Getting the best alpha from the metric list above
mat<-data.frame(Alpha=unlist(rng), R2=unlist(metric))

#Sorting the dataframe for best alpha
data_sort<-mat[order(-mat$R2),]

#Best Alpha
best_alpha<-data_sort[1,1]

#Now building a elastic net using the best alpha and evaluating the results
best_mod<-cv.glmnet(x=as.matrix(X_scale), y=as.matrix(y),
                    alpha=best_alpha,nfolds = 10)

# Plot cross-validation results
plot(best_mod)

#Lambda that gives minimum mean cross-validated error
best_mod$lambda.min

#Selected Lambda and the corresponding coefficients
coef(best_mod, s=best_mod$lambda.min)

#Now using the above variables from the Elastic net and run a new 
#regression model

new_from_elastic<-lm(Crime ~ M+So+Ed+Po1+LF+M.F+NW+U2+Ineq+Prob, 
                   data = crime_data)
summary(new_from_elastic)

#Further based on the p-value we can remove So,LF,M.F,NW and run a new model
mod_last<-lm(Crime ~ M+Ed+Po1+U2+Ineq+Prob, 
        data = crime_data)
summary(mod_last)

```
## Conclusion: Elastic Net

We used the Elastic net algorithm where the alpha value is between 0-1 for 
variable selection. The model gave 10 significant variables and these were 
subsequently used to build a regression model. Further the p-value was used to 
eliminate the insignificant variable and a final regression model was build 
which was the same as Lasso and stepwise regression.

