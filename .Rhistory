model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
#We will use the caret library to do the splitting
library(caret)
library(kknn)
library(ggplot2)
#Getting the data
credit_data<-read.table('credit_card_data-headers.txt', sep = "", header = TRUE)
#The data will be divided with 60% Train, 20% Validation and 20% Test data
set.seed(42)
#Using cresteDataPartion
train_index<-createDataPartition(y=credit_data$R1, p=0.6,
times = 1, list = FALSE)
#Training data
train_data<-credit_data[train_index,]
#Validation and testing data, this data is furturer divided into two sets
rest_data<-credit_data[-train_index,]
rest_index<-createDataPartition(y=rest_data$R1, p=0.5, times = 1,list = FALSE)
#Validation data
val_data<-rest_data[rest_index,]
#Testing data
test_data<-rest_data[-rest_index,]
#Using kkNN
acc<-list() #Creating an empty list to store accuracy
for (i in 1:50){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
View(score_sort)
View(score_sort)
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=21,
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=21,
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
for (i in 1:20){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
#We will use the caret library to do the splitting
library(caret)
library(kknn)
library(ggplot2)
#Getting the data
credit_data<-read.table('credit_card_data-headers.txt', sep = "", header = TRUE)
#The data will be divided with 60% Train, 20% Validation and 20% Test data
set.seed(42)
#Using cresteDataPartion
train_index<-createDataPartition(y=credit_data$R1, p=0.6,
times = 1, list = FALSE)
#Training data
train_data<-credit_data[train_index,]
#Validation and testing data, this data is furturer divided into two sets
rest_data<-credit_data[-train_index,]
rest_index<-createDataPartition(y=rest_data$R1, p=0.5, times = 1,list = FALSE)
#Validation data
val_data<-rest_data[rest_index,]
#Testing data
test_data<-rest_data[-rest_index,]
#Using kkNN
acc<-list() #Creating an empty list to store accuracy
for (i in 1:20){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:20), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.20, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
View(score_sort)
#We will use the caret library to do the splitting
library(caret)
detach("package:caret", unload = TRUE)
library(caret)
#We will use the caret library to do the splitting
library(caret)
library(kknn)
library(ggplot2)
#Getting the data
credit_data<-read.table('credit_card_data-headers.txt', sep = "", header = TRUE)
#The data will be divided with 60% Train, 20% Validation and 20% Test data
set.seed(42)
#Using cresteDataPartion
train_index<-createDataPartition(y=credit_data$R1, p=0.6,
times = 1, list = FALSE)
#Training data
train_data<-credit_data[train_index,]
#Validation and testing data, this data is further divided into two sets
rest_data<-credit_data[-train_index,]
rest_index<-createDataPartition(y=rest_data$R1, p=0.5, times = 1,list = FALSE)
#Validation data
val_data<-rest_data[rest_index,]
#Testing data
test_data<-rest_data[-rest_index,]
#Using kkNN
acc<-list() #Creating an empty list to store accuracy
for (i in 1:50){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
#Using Caret library
library(caret)
#Using Caret library
library(caret)
#Getting the data
credit_data<-read.table('credit_card_data-headers.txt', sep = "", header = TRUE)
#The data will be divided with 60% Train, 20% Validation and 20% Test data
set.seed(42)
#Using cresteDataPartion using the caret library
train_index<-createDataPartition(y=credit_data$R1, p=0.6,
times = 1, list = FALSE)
#Training data
train_data<-credit_data[train_index,]
#Validation and testing data, this data is further divided into two sets
rest_data<-credit_data[-train_index,]
rest_index<-createDataPartition(y=rest_data$R1, p=0.5, times = 1,list = FALSE)
#Validation data
val_data<-rest_data[rest_index,]
#Testing data
test_data<-rest_data[-rest_index,]
#Using kkNN
acc<-list() #Creating an empty list to store accuracy
#Testing the kknn method for different k values (1-50)
for (i in 1:50){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
#Choosing the best value of k from the validation accuracy. To this we put both
#accuracy and k in a data frame and plot it
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
#Getting the data
iris_data<-read.table('iris.txt', sep = "", header = TRUE)
#Visulaizing the data using various combination to get some intitution of the
#data distribution
ggplot(data = iris_data, aes(Sepal.Length,
Sepal.Width,
color=iris_data$Species))+
geom_point(alpha=1) #The plot shows two clusters with sertosa having a clean
ggplot(data = iris_data, aes(Petal.Length,
Petal.Width,
color=iris_data$Species))+
geom_point(alpha=1) #Shows three good clusters
ggplot(data = iris_data, aes(Petal.Length,
Sepal.Length,
color=iris_data$Species))+
geom_point(alpha=1) #Shows three ok clusters
ggplot(data = iris_data, aes(Petal.Width,
Sepal.Width,
color=iris_data$Species))+
geom_point(alpha=1) #Shows three good clusters
ggplot(data = iris_data, aes(Petal.Length,
Sepal.Width,
color=iris_data$Species))+
geom_point(alpha=1) #Shows three clusters
ggplot(data = iris_data, aes(Petal.Width,
Sepal.Length,
color=iris_data$Species))+
geom_point(alpha=1) #Shows three clusters
#Scaling the data
scale_iris<-scale(iris_data[-5])
#Tesing the algorithm with all the predictors for different clusters
table1<-list()
for (i in 2:5){
model_all<-kmeans(x=scale_iris[,1:4], centers = i, nstart = 30)
table1[[i]]=table(iris_data$Species,model_all$cluster)
print(table1[[i]])
}
#Tesing the algorithm with Petal Length & Petal Width for different clusters
table2<-list()
for (i in 2:5){
model_all<-kmeans(x=scale_iris[,3:4], centers = i, nstart = 30)
table2[[i]]=table(iris_data$Species,model_all$cluster)
print(table2[[i]])
}
#Tesing the algorithm with Petal Length & Petal Width for different clusters
table2<-list()
for (i in 2:5){
model_all<-kmeans(x=scale_iris[,3:4], centers = i, nstart = 30)
table2[[i]]=table(Species,model_all$cluster)
print(table2[[i]])
}
#We will use the caret library to do the splitting
library(caret)
library(kknn)
library(ggplot2)
#Getting the data
credit_data<-read.table('credit_card_data-headers.txt', sep = "", header = TRUE)
#The data will be divided with 60% Train, 20% Validation and 20% Test data
set.seed(42)
#Using cresteDataPartion
train_index<-createDataPartition(y=credit_data$R1, p=0.6,
times = 1, list = FALSE)
#Training data
train_data<-credit_data[train_index,]
#Validation and testing data, this data is further divided into two sets
rest_data<-credit_data[-train_index,]
rest_index<-createDataPartition(y=rest_data$R1, p=0.5, times = 1,list = FALSE)
#Validation data
val_data<-rest_data[rest_index,]
#Testing data
test_data<-rest_data[-rest_index,]
#Using kkNN
acc<-list() #Creating an empty list to store accuracy
for (i in 1:50){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
#Using cresteDataPartion
train_index<-createDataPartition(y=credit_data$R1, p=0.7,
times = 1, list = FALSE)
#Training data
train_data<-credit_data[train_index,]
#Validation and testing data, this data is further divided into two sets
rest_data<-credit_data[-train_index,]
rest_index<-createDataPartition(y=rest_data$R1, p=0.5, times = 1,list = FALSE)
#Validation data
val_data<-rest_data[rest_index,]
#Testing data
test_data<-rest_data[-rest_index,]
#Using kkNN
acc<-list() #Creating an empty list to store accuracy
for (i in 1:50){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
View(acc)
View(acc)
###############################################################################
#Doing cross validation manually
###############################################################################
#Importing libraries
library(kknn)
library(ggplot2)
#Reading the data
credit_data<-read.table('credit_card_data-headers.txt', sep = "", header = TRUE)
#Viewing the data
head(credit_data)
#Summarizing the data
str(credit_data)
#Setting the seed for reproducibility
set.seed(42)
#Shuffling the data randomly
credit_data<-credit_data[sample(nrow(credit_data)),]
#Creating 10 groups for cross validation. One can also user input to decide
#on tht number of cross validation groups. Will be more automated.
group <- cut(seq(1,nrow(credit_data)),breaks=10,labels=FALSE)
pre<-list() #Creating an empty list to store predictions
accu<-list() #Creating an empty list to store accuracy
for (j in 1:50){   #Loop to test different k between 1-50
acc1=0           #Setting Accuracy to 0 for new value of k
for (i in 1:10){ #Loop to test k-fold cross validation
ind<-which(group==i, arr.ind = FALSE) #This create a set using the group
model_knn = kknn(R1~.,
train=credit_data[-ind,],
test=credit_data[ind,],
k = j,
scale = TRUE)
pred_knn = fitted(model_knn)
pre[[i]]<-ifelse(pred_knn>0.5,1,0)
acc1=acc1+sum(pre[[i]] == credit_data[ind,11])/nrow(credit_data[ind,])
}
accu[[j]]=acc1/10 #Accuracy for the jth value of k
}
#Plotting k vs accuracy
dat<-data.frame(k=list(1:50), Accuracy=unlist(accu))
ggplot(data=dat,aes(x=X1.50,y=Accuracy))+geom_point(alpha=0.5,color='red')
dat_sort<-dat[order(-dat$Accuracy),]
best_k<-dat_sort[1,1]
message('Best k: ',best_k)
message('Accuracy of the model using the best k: ', dat_sort[1,2])
accu_new<-list() #Creating an empty list to store accuracy
#Model fit
for (m in 1:50){
set.seed(42)
kNN_fit<-cv.kknn(R1~.,data=credit_data,
scale=TRUE, k = m, kcv = 10)
pred<-ifelse(kNN_fit[[1]][,2]>0.5,1,0)
accu_new[[m]]=sum(pred==credit_data[,11])/nrow(credit_data)
}
#Plotting k vs accuracy
dat_new<-data.frame(k=list(1:50), Accuracy=unlist(accu_new))
ggplot(data=dat_new,aes(x=X1.50,y=Accuracy))+geom_point(alpha=0.5,color='red')
dat_sort_new<-dat_new[order(-dat_new$Accuracy),]
best_k_new<-dat_sort_new[1,1]
message('Best k: ',best_k_new)
message('Accuracy of the model using the best k: ', dat_sort_new[1,2])
?cv.kknn
kNN_fit1<-cv.kknn(R1~.,data=credit_data,
scale=TRUE, kmax = 30, kcv = 10)
kNN_fit1<-cv.kknn(R1~.,data=credit_data,
scale=TRUE, kmax = 30, kcv = 10)
###############################################################################
#Doing cross validation manually
###############################################################################
#Importing libraries
library(kknn)
kNN_fit1<-cv.kknn(R1~.,data=credit_data,
scale=TRUE, kmax = 30, kcv = 10)
kNN_fit1<-cv.kknn(R1~.,data=credit_data,
scale=TRUE, kcv = 10)
#Model fit
for (m in 1:50){
set.seed(42)
kNN_fit<-cv.kknn(R1~.,data=credit_data,
scale=TRUE, k = m, kcv = 10)
pred<-ifelse(kNN_fit[[1]][,2]>0.5,1,0)
accu_new[[m]]=sum(pred==credit_data[,11])/nrow(credit_data)
}
#Plotting k vs accuracy
dat_new<-data.frame(k=list(1:50), Accuracy=unlist(accu_new))
ggplot(data=dat_new,aes(x=X1.50,y=Accuracy))+geom_point(alpha=0.5,color='red')
dat_sort_new<-dat_new[order(-dat_new$Accuracy),]
best_k_new<-dat_sort_new[1,1]
message('Best k: ',best_k_new)
message('Accuracy of the model using the best k: ', dat_sort_new[1,2])
#We will use the caret library to do the splitting
library(caret)
library(kknn)
library(ggplot2)
#Getting the data
credit_data<-read.table('credit_card_data-headers.txt', sep = "", header = TRUE)
#The data will be divided with 70% Train, 15% Validation and 15% Test data
set.seed(42)
#Using cresteDataPartion
train_index<-createDataPartition(y=credit_data$R1, p=0.7,
times = 1, list = FALSE)
#Training data
train_data<-credit_data[train_index,]
#Validation and testing data, this data is further divided into two sets
rest_data<-credit_data[-train_index,]
rest_index<-createDataPartition(y=rest_data$R1, p=0.5, times = 1,list = FALSE)
#Validation data
val_data<-rest_data[rest_index,]
#Testing data
test_data<-rest_data[-rest_index,]
#Using kkNN
acc<-list() #Creating an empty list to store accuracy
for (i in 1:50){
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
for (i in 1:50){
set.seed(42)
model_kknn<-kknn(R1~., train = train_data, test = val_data,
k=i, scale = TRUE)
pred<-fitted(model_kknn)
pre<-ifelse(pred>0.5,1,0)
acc[[i]]=sum(pre==val_data[,11])/nrow(val_data)
}
score<-data.frame(k=list(1:50), Accuracy=unlist(acc))
ggplot(data = score, aes(x=X1.50, y=Accuracy))+geom_point(alpha=0.5,color='red')
#Finding the best model from the previous data frame
score_sort<-score[order(-score$Accuracy),]
message('Best k: ',score_sort[1,1])
message('Accuracy of the model using the best k: ', score_sort[1,2])
#Now testing this model  on the test data
model_test<-kknn(R1~.,train = train_data, test = test_data, k=score_sort[1,1],
scale = TRUE)
test_predict<-ifelse(fitted(model_test)>0.5,1,0)
test_accuracy<-sum(test_predict==test_data[,11])/nrow(test_data)
message('Accuracy of the model on the test using the best k:', test_accuracy)
?cv.kknn
test<-train.kknn(R1~., data = credit_data, kmax = 30, scale = TRUE)
test.best.parameters
View(test)
View(test)
test<-train.kknn(R1~., data = credit_data, kmax = 50, scale = TRUE)
test
test<-train.kknn(R1~., data = credit_data, kmax = 200, scale = TRUE)
test
test[[1]][,2]
test[[1]]
test[[1]][,2]
test<-cv.kknn(R1~., data = credit_data, scale = TRUE,cv=10)
test[[1]][,2]
test<-cv.kknn(R1~., data = credit_data, scale = TRUE,cv=10)
test<-cv.kknn(R1~., data = credit_data, scale = TRUE, kcv=10)
test[[1]][,2]
test[[2]][,2]
test[[1]][,2]
test<-cv.kknn(R1~., data = credit_data, scale = TRUE, kcv=16)
test[[1]][,2]
test<-cv.kknn(R1~., data = credit_data, scale = TRUE, kcv=5)
test[[1]][,2]
