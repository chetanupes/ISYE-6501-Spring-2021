#Getting the data
credit_data<-read.table('uscrime.txt', header = True)
#Getting the data
credit_data<-read.table('uscrime.txt', header = True)
#Getting the data
credit_data<-read.table('uscrime', header = True)
#Getting the data
credit_data<-read.table('uscrime.txt', sep = "", header = True)
#Getting the data
credit_data<-read.table('uscrime.txt', sep = "", header = True)
library(MASS)
#Getting the data
credit_data<-read.table('uscrime.txt', sep = "", header = True)
#Getting the training and test data
crime_data<-read.table('uscrime.txt', sep = "", header = TRUE)
#Getting the data
credit_data<-read.table('uscrime.txt', sep = "", header = TRUE)
#Getting the data
crime_data<-read.table('uscrime.txt', sep = "", header = TRUE)
#Fitting the full model
lmr<-lm(Crime~., data = crime_data)
#Getting the data
crime_data<-read.table('uscrime.txt', sep = "", header = TRUE)
#Fitting the full model
lmr<-lm(Crime~., data = crime_data)
summary(lm)
summary(lmr)
?stepAIC
back<-stepAIC(lmr, direction = 'backward',trace = 1)
back<-stepAIC(lmr, direction = 'backward',trace = 0)
back<-stepAIC(lmr, direction = 'backward',trace = 1)
back<-stepAIC(lmr, direction = 'forward',trace = 1)
summary(back)
#Set seed
set.seed(42)
back<-stepAIC(lmr, direction = 'both',trace = 1)
step(back)
back$anova
summary(back)
#Getting the data
crime_data<-read.table('uscrime.txt', sep = "", header = TRUE)
#Set seed
set.seed(42)
#Fitting the full model
lmr<-lm(Crime~., data = crime_data)
summary(lmr)
#Using all the three options for Stepwise regression and evaluating the results
#Backward
back<-stepAIC(lmr, direction = 'forward',trace = 1)
#Using all the three options for Stepwise regression and evaluating the results
#Backward
back<-stepAIC(lmr, scope = list(lower = Crime~1, upper = Crime~.),
direction = 'forward',trace = 1)
summary(back)
#Using all the three options for Stepwise regression and evaluating the results
#Backward
back<-stepAIC(lmr, scope = list(lower = Crime~1, upper = Crime~.),
direction = 'backward',trace = 1)
#Using all the three options for Stepwise regression and evaluating the results
#Backward
back<-stepAIC(lmr, scope = list(lower = Crime~1, upper = Crime~.),
direction = 'both',trace = 1)
#Using Stepwise regression and evaluating the results
#Backward
step<-stepAIC(lmr, scope = list(lower = Crime~1, upper = Crime~.),
direction = 'both',trace = 1)
#Using Stepwise regression and evaluating the results
#Backward
step<-stepAIC(lmr, scope = list(lower = Crime~1, upper = Crime~.),
direction = 'both',trace = 1)
View(step)
View(step)
step$anova
View(step)
View(step)
new<-lm(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob, data = crime_data)
#New model summary
summary(new)
set.seed(123)
train.control <- trainControl(method = "cv", number = 10)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
summary(model) # Model summary
library(caret)
set.seed(123)
train.control <- trainControl(method = "cv", number = 10)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
summary(model) # Model summary
View(model)
View(model)
set.seed(123)
train.control <- trainControl(method = "cv", number = 50)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
set.seed(123)
train.control <- trainControl(method = "cv", number = 47)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
set.seed(123)
train.control <- trainControl(method = "cv", number = 40)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
set.seed(123)
train.control <- trainControl(method = "cv", number = 20)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
summary(model) # Model summary
model$results
View(model)
View(model)
set.seed(123)
train.control <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
model$results
summary(model) # Model summary
model$resample
set.seed(123)
train.control <- trainControl(method = "cv", number = 5)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
summary(model) # Model summary
model$resample
mean(model$resample$RMSE)==model$results$RMSE
mean(model$resample$Rsquared)==model$results$Rsquared
#New model
new_1<-lm(Crime ~ M + Ed + Po1 + U2 + Ineq + Prob, data = crime_data)
#New model summary
summary(new_1)
#Linear regression model using cross validation
set.seed(123)
train.control <- trainControl(method = "cv", number = 5)
model1<-train(Crime ~ M + Ed + Po1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
# CV Model summary
summary(model1)
# CV Model summary
summary(model)
installed.packages('glmnet')
install.packages('glmnet')
library(glmnet)
?glmnet
X<-crime_data[,1:15]
View(X)
View(X)
X_scale<-scale(X)
View(X_scale)
View(X_scale)
#Lasso using CV
lasso<-cv.glmnet(x=as.matrix(X_scale), y = as.matrix(y), alpha=1)
?cv.glmnet
#Lasso using CV
lasso<-cv.glmnet(x=as.matrix(X_scale), y = as.matrix(y), alpha=1)
#Lasso using CV
lasso<-cv.glmnet(x=as.matrix(X_scale), y = y, alpha=1)
y<-crime_data[16]
#We need to also scale the data
X_scale<-scale(X)
#Lasso using CV
lasso<-cv.glmnet(x=as.matrix(X_scale), y = y, alpha=1)
#Lasso using CV
lasso<-cv.glmnet(x=as.matrix(X_scale), y = as.matrix(y), alpha=1)
summary(lasso)
plot(lasso)
lasso$lambda.min
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
#Let first separate response and predictor for the algorithm
X<-crime_data[,1:15]
y<-crime_data[16]
#We need to also scale the data
X_scale<-scale(X)
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length = 100)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = 10,
standardize = TRUE, nfolds = 10)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Plot cross-validation results
plot(lasso_cv)
lasso_cv$lambda.min
coef(lasso_cv)
coef(lasso_cv, s=lasso_cv$lambda.min)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
standardize = TRUE, nfolds = 10)
# Plot cross-validation results
plot(lasso_cv)
lasso_cv$lambda.min
coef(lasso_cv, s=lasso_cv$lambda.min)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 10)
# Plot cross-validation results
plot(lasso_cv)
lasso_cv$lambda.min
coef(lasso_cv, s=lasso_cv$lambda.min)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5)
# Plot cross-validation results
plot(lasso_cv)
lasso_cv$lambda.min
coef(lasso_cv, s=lasso_cv$lambda.min)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
lasso_cv$lambda.min
coef(lasso_cv, s=lasso_cv$lambda.min)
coef(lasso_cv)
View(y)
View(y)
coef(lasso_cv, s=lasso_cv$lambda.min)
View(lasso_cv)
View(lasso_cv)
coef(lasso_cv, s=lasso_cv$lambda.1se)
coef(lasso_cv, s=lasso_cv$lambda.min,exact=TRUE)
coef(lasso_cv, s=lasso_cv$lambda.min,exact=TRUE)
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(10, -2, length = 100)
# Setting alpha = 1 implements ridge regression
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1, lambda = lambdas_to_try,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
lasso_cv$lambda.min
coef(lasso_cv, s=lasso_cv$lambda.min,exact=TRUE)
coef(lasso_cv, s=lasso_cv$lambda.1se,exact=TRUE)
coef(lasso_cv, s=lasso_cv$lambda.min,exact=TRUE)
coef(lasso_cv, s = "lambda.min")
lasso_cv$lambda.min
# Setting alpha = 1 implements Lasso regression
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
lasso_cv$lambda.min
coef(lasso_cv, s = "lambda.min")
coef(lasso_cv, s=lasso_cv$lambda.min)
lasso_cv$lambda
library(MASS)
library(caret)
library(glmnet)
#Getting the data
crime_data<-read.table('uscrime.txt', sep = "", header = TRUE)
#Set seed
set.seed(42)
#Fitting the full model
lmr<-lm(Crime~., data = crime_data)
summary(lmr)
#Using Stepwise regression and evaluating the results
step<-stepAIC(lmr, scope = list(lower = Crime~1, upper = Crime~.),
direction = 'both',trace = 1)
#Model summary
summary(step)
step$anova
#New model
new<-lm(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob, data = crime_data)
#New model summary
summary(new)
#Now to further evaluate this model let use cross validation
#Linear regression model using cross validation
set.seed(123)
train.control <- trainControl(method = "cv", number = 5)
model<-train(Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
# CV Model summary
summary(model)
#New model
new_1<-lm(Crime ~ M + Ed + Po1 + U2 + Ineq + Prob, data = crime_data)
#New model summary
summary(new_1)
#Now to further evaluate this model let use cross validation
#Linear regression model using cross validation
set.seed(123)
train.control <- trainControl(method = "cv", number = 5)
model1<-train(Crime ~ M + Ed + Po1 + U2 + Ineq + Prob,
data = crime_data, method='lm',  #Linear Model2 with CV
trControl=train.control)
# CV Model summary
summary(model1)
#Let first separate response and predictor for the algorithm
X<-crime_data[,1:15]
y<-crime_data[16]
#We need to also scale the data
X_scale<-scale(X)
# Setting alpha = 1 implements Lasso regressions
set.seed(42)
set.seed(42)
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
#λ that gives minimum mean cross-validated error
lasso_cv$lambda.min
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.min)
set.seed(42)
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
#λ that gives minimum mean cross-validated error
lasso_cv$lambda.min
set.seed(40)
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
#λ that gives minimum mean cross-validated error
lasso_cv$lambda.min
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.min)
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.1se)
# Setting alpha = 1 implements Lasso regressions
set.seed(42)
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
#λ that gives minimum mean cross-validated error
lasso_cv$lambda.min
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.1se)
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.min)
data <- read.table("uscrime.txt", stringsAsFactors = FALSE, header = TRUE)
scaledData = as.data.frame(scale(data[,c(1,3,4,5,6,7,8,9,10,11,12,13,14,15)]))
scaledData <- cbind(data[,2],scaledData,data[,16]) # Add column 2 back in
colnames(scaledData)[1] <- "So"
colnames(scaledData)[16] <- "Crime"
View(scaledData)
View(scaledData)
View(X)
View(X)
View(X_scale)
View(X_scale)
#We need to also scale the data except the response data
X_scale<-scale(X[-2])
View(X_scale)
View(X_scale)
X-scale<-cbind(X[2], X_scale)
X-scale<-cbind(X[,2], X_scale)
#We need to also scale the data except the response data
X_sca<-scale(X[-2])
X-scale<-cbind(X[,2], X_sca)
X_scale<-cbind(X[,2], X_sca)
View(X_scale)
View(X_scale)
set.seed(42)
lasso_cv <- cv.glmnet(x=as.matrix(X), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
#λ that gives minimum mean cross-validated error
lasso_cv$lambda.min
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.min)
lasso_cv <- cv.glmnet(x=as.matrix(X_scale), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
#λ that gives minimum mean cross-validated error
lasso_cv$lambda.min
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.min)
#We need to also scale the data except the response data
X_scale<-scale(X)
# Setting alpha = 1 implements Lasso regressions
set.seed(42)
lasso_cv <- cv.glmnet(x=as.matrix(X_scale), y=as.matrix(y), alpha = 1,
nfolds = 5,type.measure="mse",family="gaussian")
# Plot cross-validation results
plot(lasso_cv)
#λ that gives minimum mean cross-validated error
lasso_cv$lambda.min
#Selected λ and the corresponding coefficients
coef(lasso_cv, s=lasso_cv$lambda.min)
lasso_cv$lambda
summary(new_from_lasso)
new_from_lasso<-lm(Crime ~ M+So+Ed+Po1+M.F+NW+U1+U2+Ineq+Prob+Wealth,
data = crime_data)
summary(new_from_lasso)
#Further based on the p-value we can remove So,M.F,NW,U1,Wealth and run a new model
mod<-lm(Crime ~ M+Ed+Po1+U2+Ineq+Prob,
data = crime_data)
summary(lm)
summary(mod)
